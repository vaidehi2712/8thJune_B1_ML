{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b391ad6-61a8-49c6-be32-2d01cf36558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '''This article is about natural language processing done by computers. For the natural language\n",
    "processing done by the human brain, see Language processing in the brain.\n",
    "An automated online assistant providing customer service on a web page, an example of an application w\n",
    "here natural language processing is a major component.\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intel\n",
    "ligence concerned with the interactions between computers and human language, in particular how to pro\n",
    "gram computers to process and analyze large amounts of natural language data. The goal is a computer c\n",
    "apable of \"understanding\" the contents of documents, including the contextual nuances of the language\n",
    " within them. The technology can then accurately extract information and insights contained in the doc\n",
    "uments as well as categorize and organize the documents themselves.\n",
    "Challenges in natural language processing frequently involve speech recognition, natural language unde\n",
    "rstanding, and natural language generation.\n",
    "you would've had done that, isn\\'t it?\n",
    "'''.strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9326f2-6b9f-48c3-a97b-35d2866dab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article is about natural language processing done by computers. For the natural language\n",
      "processing done by the human brain, see Language processing in the brain.\n",
      "An automated online assistant providing customer service on a web page, an example of an application w\n",
      "here natural language processing is a major component.\n",
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intel\n",
      "ligence concerned with the interactions between computers and human language, in particular how to pro\n",
      "gram computers to process and analyze large amounts of natural language data. The goal is a computer c\n",
      "apable of \"understanding\" the contents of documents, including the contextual nuances of the language\n",
      " within them. The technology can then accurately extract information and insights contained in the doc\n",
      "uments as well as categorize and organize the documents themselves.\n",
      "Challenges in natural language processing frequently involve speech recognition, natural language unde\n",
      "rstanding, and natural language generation.\n",
      "you would've had done that, isn't it?\n"
     ]
    }
   ],
   "source": [
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c21494-d3a7-4074-b97e-406c86e535f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46bf7a9b-60ff-4802-81f6-4d47b5b9ca48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8765b20-c754-40d7-8303-02fccc4775cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This article is about natural language processing done by computers.',\n",
       " 'For the natural language\\nprocessing done by the human brain, see Language processing in the brain.',\n",
       " 'An automated online assistant providing customer service on a web page, an example of an application w\\nhere natural language processing is a major component.',\n",
       " 'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intel\\nligence concerned with the interactions between computers and human language, in particular how to pro\\ngram computers to process and analyze large amounts of natural language data.',\n",
       " 'The goal is a computer c\\napable of \"understanding\" the contents of documents, including the contextual nuances of the language\\n within them.',\n",
       " 'The technology can then accurately extract information and insights contained in the doc\\numents as well as categorize and organize the documents themselves.',\n",
       " 'Challenges in natural language processing frequently involve speech recognition, natural language unde\\nrstanding, and natural language generation.',\n",
       " \"you would've had done that, isn't it?\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = nltk.sent_tokenize(txt)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3784a9b7-55af-4758-9348-f94f03926ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be4a4d2c-2bdb-478d-a02a-4d02aef24572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\hp\\anaconda3\\lib\\site-packages (21.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceb63a78-75c0-40fe-836b-b67d5b0e23cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'article',\n",
       " 'is',\n",
       " 'about',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'done',\n",
       " 'by',\n",
       " 'computers',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(sents[0])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "851bbe42-3668-4b83-8be9-9ab0aedf85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation as punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b027309f-d29e-45df-a5ab-1f903168e103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c323636-7fad-4374-b365-37914de6c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "051c1873-2784-4c2c-aabf-89e6800d0fb9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swe = sw.words('english')\n",
    "swe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "641d375d-9406-4437-bf4e-e26b032d1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt2 = '''Given a stream of text, determine which items in the text map to proper names, such as peopl\n",
    "e or places, and what the type of each such name is (e.g. person, location, organization). Although ca\n",
    "pitalization can aid in recognizing named entities in languages such as English, this information cann\n",
    "ot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient.\n",
    " For example, the first letter of a sentence is also capitalized, and named entities often span severa\n",
    "l words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts\n",
    " (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalizatio\n",
    "n may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardl\n",
    "ess of whether they are names, and French and Spanish do not capitalize names that serve as adjective\n",
    "s.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50d9c0d6-bb40-4898-a1fd-40c49f892872",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt3 = '''Given a sentence or larger chunk of text, determine which words (\"mentions\") refer to the sa\n",
    "me objects (\"entities\"). Anaphora resolution is a specific example of this task, and is specifically c\n",
    "oncerned with matching up pronouns with the nouns or names to which they refer. The more general task\n",
    " of coreference resolution also includes identifying so-called \"bridging relationships\" involving refe\n",
    "rring expressions. For example, in a sentence such as \"He entered John's house through the front doo\n",
    "r\", \"the front door\" is a referring expression and the bridging relationship to be identified is the f\n",
    "act that the door being referred to is the front door of John's house (rather than of some other struc\n",
    "ture that might also be referred to).'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ea09a76-4796-43c4-989f-b10a39ba07d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(t):\n",
    "    words = []\n",
    "    for sent in nltk.sent_tokenize(t.lower()): \n",
    "        ws = nltk.word_tokenize(sent)\n",
    "        ws = [w for w in ws if w.isalpha() and w not in swe]\n",
    "        #ws = [w for w in ws if w not in punc] \n",
    "        words+=(ws)\n",
    "    words = ' '.join(words)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d25fc925-ff68-450d-956c-aa41506f521d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article natural language processing done computers natural language processing done human brain see language processing brain automated online assistant providing customer service web page example application w natural language processing major component natural language processing nlp subfield linguistics computer science artificial intel ligence concerned interactions computers human language particular pro gram computers process analyze large amounts natural language data goal computer c apable understanding contents documents including contextual nuances language within technology accurately extract information insights contained doc uments well categorize organize documents challenges natural language processing frequently involve speech recognition natural language unde rstanding natural language generation would done',\n",
       " 'given stream text determine items text map proper names peopl e places type name person location organization although ca pitalization aid recognizing named entities languages english information cann ot aid determining type named entity case often inaccurate insufficient example first letter sentence also capitalized named entities often span severa l words capitalized furthermore many languages scripts chinese arabic capitalization even languages capitalizatio n may consistently use distinguish names example german capitalizes nouns regardl ess whether names french spanish capitalize names serve adjective',\n",
       " 'given sentence larger chunk text determine words mentions refer sa objects entities anaphora resolution specific example task specifically c oncerned matching pronouns nouns names refer general task coreference resolution also includes identifying bridging relationships involving refe rring expressions example sentence entered john house front doo r front door referring expression bridging relationship identified f act door referred front door john house rather struc ture might also referred']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = []\n",
    "word_list.append(preprocess(txt))\n",
    "word_list.append(preprocess(txt2))\n",
    "word_list.append(preprocess(txt3))\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fb46ee5-3aa5-4710-93af-0023cf391461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed3772d4-8884-4ff8-af9e-f8cbdbdc22cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7091eaca-095a-42e3-aed4-d806949848ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('its', 'PRP$'),\n",
       " ('better', 'NN'),\n",
       " ('you', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('go', 'VB'),\n",
       " ('there', 'RB')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = nltk.pos_tag(nltk.word_tokenize('its better you do not go there'))\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aab79508-df56-4723-9d0b-fc72d6d534f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('its', 'PRP$'),\n",
       " ('better', 'NN'),\n",
       " ('you', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('go', 'VB'),\n",
       " ('there', 'RB')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f9ce240-1b29-41a5-b45d-c74429e121c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('better',wordnet.ADV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54c670c8-e77f-42d7-bb98-35cf9274e0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de77be48-1e73-47df-8369-3a068196489c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.ADJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d50be81b-b8ef-4f5c-9451-ff45ba791926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('better',wordnet.ADJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f87d809-cb58-4d24-852b-5df1ed715c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'better'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " lemma.lemmatize('better',wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0c8de05-e68d-4bfe-b5fb-556a6a23c90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('going')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0252b4e6-1b0a-4018-aff0-eacec3a3f530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doe'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('does')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e9d4749-7b58-408a-b18e-4946ee0272b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('worked')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dd4447-7884-403f-8143-efc935627aa9",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9357042-5404-4039-9e74-d261fa3bfc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08a9d1e9-ebd9-4e04-9d90-1104eecbe030",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8dc21d4-efd2-4807-9606-05e8c9491d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = cv.fit_transform(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e1e3373-228a-4ea3-bf99-be5941cb5707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x171 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 183 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe168c1e-cf00-4106-91ee-ab45b21ce165",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accurately',\n",
       " 'act',\n",
       " 'adjective',\n",
       " 'aid',\n",
       " 'also',\n",
       " 'although',\n",
       " 'amounts',\n",
       " 'analyze',\n",
       " 'anaphora',\n",
       " 'apable',\n",
       " 'application',\n",
       " 'arabic',\n",
       " 'article',\n",
       " 'artificial',\n",
       " 'assistant',\n",
       " 'automated',\n",
       " 'brain',\n",
       " 'bridging',\n",
       " 'ca',\n",
       " 'cann',\n",
       " 'capitalizatio',\n",
       " 'capitalization',\n",
       " 'capitalize',\n",
       " 'capitalized',\n",
       " 'capitalizes',\n",
       " 'case',\n",
       " 'categorize',\n",
       " 'challenges',\n",
       " 'chinese',\n",
       " 'chunk',\n",
       " 'component',\n",
       " 'computer',\n",
       " 'computers',\n",
       " 'concerned',\n",
       " 'consistently',\n",
       " 'contained',\n",
       " 'contents',\n",
       " 'contextual',\n",
       " 'coreference',\n",
       " 'customer',\n",
       " 'data',\n",
       " 'determine',\n",
       " 'determining',\n",
       " 'distinguish',\n",
       " 'doc',\n",
       " 'documents',\n",
       " 'done',\n",
       " 'doo',\n",
       " 'door',\n",
       " 'english',\n",
       " 'entered',\n",
       " 'entities',\n",
       " 'entity',\n",
       " 'ess',\n",
       " 'even',\n",
       " 'example',\n",
       " 'expression',\n",
       " 'expressions',\n",
       " 'extract',\n",
       " 'first',\n",
       " 'french',\n",
       " 'frequently',\n",
       " 'front',\n",
       " 'furthermore',\n",
       " 'general',\n",
       " 'generation',\n",
       " 'german',\n",
       " 'given',\n",
       " 'goal',\n",
       " 'gram',\n",
       " 'house',\n",
       " 'human',\n",
       " 'identified',\n",
       " 'identifying',\n",
       " 'inaccurate',\n",
       " 'includes',\n",
       " 'including',\n",
       " 'information',\n",
       " 'insights',\n",
       " 'insufficient',\n",
       " 'intel',\n",
       " 'interactions',\n",
       " 'involve',\n",
       " 'involving',\n",
       " 'items',\n",
       " 'john',\n",
       " 'language',\n",
       " 'languages',\n",
       " 'large',\n",
       " 'larger',\n",
       " 'letter',\n",
       " 'ligence',\n",
       " 'linguistics',\n",
       " 'location',\n",
       " 'major',\n",
       " 'many',\n",
       " 'map',\n",
       " 'matching',\n",
       " 'may',\n",
       " 'mentions',\n",
       " 'might',\n",
       " 'name',\n",
       " 'named',\n",
       " 'names',\n",
       " 'natural',\n",
       " 'nlp',\n",
       " 'nouns',\n",
       " 'nuances',\n",
       " 'objects',\n",
       " 'often',\n",
       " 'oncerned',\n",
       " 'online',\n",
       " 'organization',\n",
       " 'organize',\n",
       " 'ot',\n",
       " 'page',\n",
       " 'particular',\n",
       " 'peopl',\n",
       " 'person',\n",
       " 'pitalization',\n",
       " 'places',\n",
       " 'pro',\n",
       " 'process',\n",
       " 'processing',\n",
       " 'pronouns',\n",
       " 'proper',\n",
       " 'providing',\n",
       " 'rather',\n",
       " 'recognition',\n",
       " 'recognizing',\n",
       " 'refe',\n",
       " 'refer',\n",
       " 'referred',\n",
       " 'referring',\n",
       " 'regardl',\n",
       " 'relationship',\n",
       " 'relationships',\n",
       " 'resolution',\n",
       " 'rring',\n",
       " 'rstanding',\n",
       " 'sa',\n",
       " 'science',\n",
       " 'scripts',\n",
       " 'see',\n",
       " 'sentence',\n",
       " 'serve',\n",
       " 'service',\n",
       " 'severa',\n",
       " 'span',\n",
       " 'spanish',\n",
       " 'specific',\n",
       " 'specifically',\n",
       " 'speech',\n",
       " 'stream',\n",
       " 'struc',\n",
       " 'subfield',\n",
       " 'task',\n",
       " 'technology',\n",
       " 'text',\n",
       " 'ture',\n",
       " 'type',\n",
       " 'uments',\n",
       " 'unde',\n",
       " 'understanding',\n",
       " 'use',\n",
       " 'web',\n",
       " 'well',\n",
       " 'whether',\n",
       " 'within',\n",
       " 'words',\n",
       " 'would']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uw = cv.get_feature_names()\n",
    "uw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57ad0e21-1c28-40b3-ae92-cf181fbfb5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accurately</th>\n",
       "      <th>act</th>\n",
       "      <th>adjective</th>\n",
       "      <th>aid</th>\n",
       "      <th>also</th>\n",
       "      <th>although</th>\n",
       "      <th>amounts</th>\n",
       "      <th>analyze</th>\n",
       "      <th>anaphora</th>\n",
       "      <th>apable</th>\n",
       "      <th>...</th>\n",
       "      <th>uments</th>\n",
       "      <th>unde</th>\n",
       "      <th>understanding</th>\n",
       "      <th>use</th>\n",
       "      <th>web</th>\n",
       "      <th>well</th>\n",
       "      <th>whether</th>\n",
       "      <th>within</th>\n",
       "      <th>words</th>\n",
       "      <th>would</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accurately  act  adjective  aid  also  although  amounts  analyze  \\\n",
       "0           1    0          0    0     0         0        1        1   \n",
       "1           0    0          1    2     1         1        0        0   \n",
       "2           0    1          0    0     2         0        0        0   \n",
       "\n",
       "   anaphora  apable  ...  uments  unde  understanding  use  web  well  \\\n",
       "0         0       1  ...       1     1              1    0    1     1   \n",
       "1         0       0  ...       0     0              0    1    0     0   \n",
       "2         1       0  ...       0     0              0    0    0     0   \n",
       "\n",
       "   whether  within  words  would  \n",
       "0        0       1      0      1  \n",
       "1        1       0      1      0  \n",
       "2        0       0      1      0  \n",
       "\n",
       "[3 rows x 171 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df = pd.DataFrame(bow.toarray(), columns=uw)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "531abf43-ac9d-488c-a391-01c229bd3c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b33e1f6d-d0e7-48db-a4e4-39f668c6d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfV = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1bf0642-86e3-4fda-b717-de108b80ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tfidfV.fit_transform(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bbdcf4b-0abd-43f8-9116-2e4052029937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x171 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 183 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "250c8428-3b40-4a94-bc2a-db33244b3b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "uw1 = tfidfV.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83eabf51-6ca1-4a75-8c1b-d6de01ca6589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accurately</th>\n",
       "      <th>act</th>\n",
       "      <th>adjective</th>\n",
       "      <th>aid</th>\n",
       "      <th>also</th>\n",
       "      <th>although</th>\n",
       "      <th>amounts</th>\n",
       "      <th>analyze</th>\n",
       "      <th>anaphora</th>\n",
       "      <th>apable</th>\n",
       "      <th>...</th>\n",
       "      <th>uments</th>\n",
       "      <th>unde</th>\n",
       "      <th>understanding</th>\n",
       "      <th>use</th>\n",
       "      <th>web</th>\n",
       "      <th>well</th>\n",
       "      <th>whether</th>\n",
       "      <th>within</th>\n",
       "      <th>words</th>\n",
       "      <th>would</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.05653</td>\n",
       "      <td>0.05653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.05653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05653</td>\n",
       "      <td>0.05653</td>\n",
       "      <td>0.05653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.05653</td>\n",
       "      <td>0.05653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.05653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.05653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099344</td>\n",
       "      <td>0.198687</td>\n",
       "      <td>0.075553</td>\n",
       "      <td>0.099344</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.099344</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.099344</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.075553</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.107167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.107167</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.081504</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accurately       act  adjective       aid      also  although  amounts  \\\n",
       "0     0.05653  0.000000   0.000000  0.000000  0.000000  0.000000  0.05653   \n",
       "1     0.00000  0.000000   0.099344  0.198687  0.075553  0.099344  0.00000   \n",
       "2     0.00000  0.107167   0.000000  0.000000  0.163007  0.000000  0.00000   \n",
       "\n",
       "   analyze  anaphora   apable  ...   uments     unde  understanding       use  \\\n",
       "0  0.05653  0.000000  0.05653  ...  0.05653  0.05653        0.05653  0.000000   \n",
       "1  0.00000  0.000000  0.00000  ...  0.00000  0.00000        0.00000  0.099344   \n",
       "2  0.00000  0.107167  0.00000  ...  0.00000  0.00000        0.00000  0.000000   \n",
       "\n",
       "       web     well   whether   within     words    would  \n",
       "0  0.05653  0.05653  0.000000  0.05653  0.000000  0.05653  \n",
       "1  0.00000  0.00000  0.099344  0.00000  0.075553  0.00000  \n",
       "2  0.00000  0.00000  0.000000  0.00000  0.081504  0.00000  \n",
       "\n",
       "[3 rows x 171 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(tf_idf.toarray(), columns=uw1)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb862ee-30c2-44b9-87cb-502bd2a625d3",
   "metadata": {},
   "source": [
    "'SMS Spam Prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a935f267-cace-46c5-a16c-b5d66d974ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                                msg\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('SMSSpamCollection.tsv',sep='\\t',names=['type','msg'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13b0fc69-e224-47d3-826c-d27ef011791a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">msg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4822</td>\n",
       "      <td>4513</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>746</td>\n",
       "      <td>652</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       msg                                                               \n",
       "     count unique                                                top freq\n",
       "type                                                                     \n",
       "ham   4822   4513                             Sorry, I'll call later   30\n",
       "spam   746    652  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('type').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1bd74e3-45a7-46d8-bf72-049792166e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5568 entries, 0 to 5567\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   type    5568 non-null   object\n",
      " 1   msg     5568 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.1+ KB\n"
     ]
    }
   ],
   "source": [
    " data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef5718e0-df20-45c7-b57d-756dc4869cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\n"
     ]
    }
   ],
   "source": [
    "for ind, m in data.iterrows():\n",
    "    print(m['msg'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "281b7d64-1a5c-4d94-9f2f-17dd6684fa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for _, m in data.iterrows():\n",
    "    word_list.append(preprocess(m['msg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "25ad1c4a-f28f-4adf-b114-ec1a6075c2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5567"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d68e797-906a-4704-8194-644f46b02e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfV = TfidfVectorizer()\n",
    "tf_idf = tfidfV.fit_transform(word_list)\n",
    "uw1 = tfidfV.get_feature_names()\n",
    "tfidf_df = pd.DataFrame(tf_idf.toarray(), columns=uw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f099a13-b9fb-4d13-acab-ea8ae45ad300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aah</th>\n",
       "      <th>aaniye</th>\n",
       "      <th>aaooooright</th>\n",
       "      <th>aathi</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abeg</th>\n",
       "      <th>abel</th>\n",
       "      <th>...</th>\n",
       "      <th>zebra</th>\n",
       "      <th>zed</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5568 rows × 7194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa  aah  aaniye  aaooooright  aathi   ab  abbey  abdomen  abeg  abel  \\\n",
       "0     0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "1     0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "2     0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "3     0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "4     0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "...   ...  ...     ...          ...    ...  ...    ...      ...   ...   ...   \n",
       "5563  0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "5564  0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "5565  0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "5566  0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "5567  0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "\n",
       "      ...  zebra  zed  zeros  zhong  zindgi  zoe  zogtorius  zoom  zouk  zyada  \n",
       "0     ...    0.0  0.0    0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "1     ...    0.0  0.0    0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "2     ...    0.0  0.0    0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "3     ...    0.0  0.0    0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "4     ...    0.0  0.0    0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "...   ...    ...  ...    ...    ...     ...  ...        ...   ...   ...    ...  \n",
       "5563  ...    0.0  0.0    0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "5564  ...    0.0  0.0    0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "5565  ...    0.0  0.0    0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "5566  ...    0.0  0.0    0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "5567  ...    0.0  0.0    0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "\n",
       "[5568 rows x 7194 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f92eada-a339-4122-865a-1d0d83a42566",
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1e0fb497-9823-4f2c-928e-5949d6256ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(tfidf_df, data['type'], test_size=0.25, random_state=85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b3b2f657-72a9-4b46-96d5-c7eeaeb5d253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB: 0.9647988505747126\n",
      "\n",
      "MultinomialNB: 0.9561781609195402\n",
      "\n",
      "GaussianNB: 0.8721264367816092\n"
     ]
    }
   ],
   "source": [
    "model_b = BernoulliNB().fit(xtrain,ytrain)\n",
    "print('BernoulliNB:',model_b.score(xtest,ytest))\n",
    "print('')\n",
    "\n",
    "model_m = MultinomialNB().fit(xtrain,ytrain)\n",
    "print('MultinomialNB:',model_m.score(xtest,ytest))\n",
    "print('')\n",
    "\n",
    "model_g = GaussianNB().fit(xtrain,ytrain)\n",
    "print('GaussianNB:',model_g.score(xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ac6a7-c21d-4a3d-8519-3f174c995136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
